{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dba0cff54e074af0beab116f0a12579d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e439212c60a4a61b3805f84b9382774",
              "IPY_MODEL_4547c8034e2f449199575ca9b207cf88",
              "IPY_MODEL_d740242345ee431098a81512deae7e03"
            ],
            "layout": "IPY_MODEL_f628d2b806da42d2a30947cdccaa873b"
          }
        },
        "8e439212c60a4a61b3805f84b9382774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c774a2a4821460db289684c18c92153",
            "placeholder": "​",
            "style": "IPY_MODEL_43c4522a4e5d481db29f64a2f02cf4a7",
            "value": "Map: 100%"
          }
        },
        "4547c8034e2f449199575ca9b207cf88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2fdecbe44934077b70bd220f307e622",
            "max": 1190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b215aaa9f3542328f78ae15c0494919",
            "value": 1190
          }
        },
        "d740242345ee431098a81512deae7e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5de81a8fa6e45dc8c484f103a997114",
            "placeholder": "​",
            "style": "IPY_MODEL_aa747ec54d7b4cac816c4bd0e3fcdbaf",
            "value": " 1190/1190 [00:01&lt;00:00, 792.46 examples/s]"
          }
        },
        "f628d2b806da42d2a30947cdccaa873b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c774a2a4821460db289684c18c92153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c4522a4e5d481db29f64a2f02cf4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2fdecbe44934077b70bd220f307e622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b215aaa9f3542328f78ae15c0494919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5de81a8fa6e45dc8c484f103a997114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa747ec54d7b4cac816c4bd0e3fcdbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iGYyXQfsbJ5f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator,\n",
        ")\n",
        "import evaluate\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "PAD_ON_RIGHT = True\n",
        "SEED = 42\n",
        "\n",
        "LANGS = [\"en\", \"es\", \"hi\", \"de\", \"ar\", \"ru\", \"vi\", \"zh\", \"tr\", \"th\"]\n",
        "\n",
        "SPLIT_RATIO = 0.9\n",
        "\n"
      ],
      "metadata": {
        "id": "WpV72VZ9bVbU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "def load_xquad_multilingual(langs):\n",
        "    pieces = []\n",
        "    for lang in langs:\n",
        "        cfg = f\"xquad.{lang}\"\n",
        "        ds = load_dataset(\"xquad\", cfg)\n",
        "        ds = ds[\"validation\"].train_test_split(test_size=0.1, seed=42)\n",
        "        ds = DatasetDict(train=ds[\"train\"], validation=ds[\"test\"])\n",
        "        pieces.append(ds)\n",
        "\n",
        "    train_all = pieces[0][\"train\"]\n",
        "    valid_all = pieces[0][\"validation\"]\n",
        "\n",
        "    for part in pieces[1:]:\n",
        "        train_all = concatenate_datasets([train_all, part[\"train\"]])\n",
        "        valid_all = concatenate_datasets([valid_all, part[\"validation\"]])\n",
        "\n",
        "    return DatasetDict(train=train_all, validation=valid_all)\n",
        "\n",
        "raw_datasets = load_xquad_multilingual(LANGS)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY2q-XZ8bZzy",
        "outputId": "43f2266c-bf50-4219-826b-23c34e716497"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer / Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U95PY0RGbeGG",
        "outputId": "7669275d-9aa9-484e-8cb6-e2dc3359434e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def prepare_train_features(examples):\n",
        "    questions = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        questions if PAD_ON_RIGHT else examples[\"context\"],\n",
        "        examples[\"context\"] if PAD_ON_RIGHT else questions,\n",
        "        truncation=\"only_second\" if PAD_ON_RIGHT else \"only_first\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        sample_idx = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_idx]\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "            continue\n",
        "\n",
        "        start_char = answers[\"answer_start\"][0]\n",
        "        answer_text = answers[\"text\"][0]\n",
        "        end_char = start_char + len(answer_text)\n",
        "\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "\n",
        "        context_index = 1 if PAD_ON_RIGHT else 0\n",
        "\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != context_index:\n",
        "            token_start_index += 1\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != context_index:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        if not (offsets[token_start_index][0] <= start_char and\n",
        "                offsets[token_end_index][1] >= end_char):\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            start_positions.append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            end_positions.append(token_end_index + 1)\n",
        "\n",
        "    tokenized[\"start_positions\"] = start_positions\n",
        "    tokenized[\"end_positions\"] = end_positions\n",
        "    return tokenized\n",
        "\n"
      ],
      "metadata": {
        "id": "uZf7iadAbi27"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_validation_features(examples):\n",
        "    questions = [q.lstrip() for q in examples[\"question\"]]\n",
        "    tokenized = tokenizer(\n",
        "        questions if PAD_ON_RIGHT else examples[\"context\"],\n",
        "        examples[\"context\"] if PAD_ON_RIGHT else questions,\n",
        "        truncation=\"only_second\" if PAD_ON_RIGHT else \"only_first\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    tokenized[\"example_id\"] = []\n",
        "    for i in range(len(tokenized[\"input_ids\"])):\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        context_index = 1 if PAD_ON_RIGHT else 0\n",
        "\n",
        "        sample_idx = sample_mapping[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        offsets = tokenized[\"offset_mapping\"][i]\n",
        "        tokenized[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(offsets)\n",
        "        ]\n",
        "        tokenized[\"example_id\"].append(examples[\"id\"][sample_idx])\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "train_dataset = raw_datasets[\"train\"].map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "eval_examples = raw_datasets[\"validation\"]\n",
        "eval_dataset = eval_examples.map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")\n",
        "\n",
        "# Metrics (SQuAD-style EM/F1)\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "def postprocess_qa_predictions(\n",
        "    examples,\n",
        "    features,\n",
        "    predictions: Tuple[np.ndarray, np.ndarray],\n",
        "    n_best_size: int = 20,\n",
        "    max_answer_length: int = 30,\n",
        "):\n",
        "    all_start_logits, all_end_logits = predictions\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = {}\n",
        "    for i, f in enumerate(features):\n",
        "        example_id = f[\"example_id\"]\n",
        "        features_per_example.setdefault(example_id, []).append(i)\n",
        "\n",
        "    predictions = {}\n",
        "    for example_id, feature_indices in features_per_example.items():\n",
        "        context = examples[example_id_to_index[example_id]][\"context\"]\n",
        "\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "\n",
        "        for fi in feature_indices:\n",
        "            start_logits = all_start_logits[fi]\n",
        "            end_logits = all_end_logits[fi]\n",
        "            offsets = features[fi][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logits)[-1:-n_best_size-1:-1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1:-n_best_size-1:-1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if start_index >= len(offsets) or end_index >= len(offsets):\n",
        "                        continue\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offsets[start_index][0]\n",
        "                    end_char = offsets[end_index][1]\n",
        "                    text = context[start_char:end_char]\n",
        "                    score = start_logits[start_index] + end_logits[end_index]\n",
        "                    valid_answers.append({\"text\": text, \"score\": score})\n",
        "\n",
        "        if valid_answers:\n",
        "            best_answer = max(valid_answers, key=lambda x: x[\"score\"])\n",
        "            predictions[example_id] = best_answer[\"text\"]\n",
        "        else:\n",
        "            predictions[example_id] = \"\"\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = postprocess_qa_predictions(\n",
        "        eval_examples, eval_dataset, p.predictions\n",
        "    )\n",
        "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in eval_examples]\n",
        "    return squad_metric.compute(predictions=[{\"id\": k, \"prediction_text\": v} for k, v in preds.items()],\n",
        "                                references=references)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dba0cff54e074af0beab116f0a12579d",
            "8e439212c60a4a61b3805f84b9382774",
            "4547c8034e2f449199575ca9b207cf88",
            "d740242345ee431098a81512deae7e03",
            "f628d2b806da42d2a30947cdccaa873b",
            "3c774a2a4821460db289684c18c92153",
            "43c4522a4e5d481db29f64a2f02cf4a7",
            "b2fdecbe44934077b70bd220f307e622",
            "1b215aaa9f3542328f78ae15c0494919",
            "b5de81a8fa6e45dc8c484f103a997114",
            "aa747ec54d7b4cac816c4bd0e3fcdbaf"
          ]
        },
        "id": "YGkfg3iUbmXZ",
        "outputId": "b24a9021-3a12-4cdd-99c9-8bbb0adee20b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1190 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dba0cff54e074af0beab116f0a12579d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./mbert-xquad\",\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    logging_steps=200,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "data_collator = default_data_collator\n",
        "\n"
      ],
      "metadata": {
        "id": "zoxZZpwUboc1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Evaluation:\", metrics)\n",
        "\n",
        "trainer.save_model(\"./mbert-xquad/final\")\n",
        "tokenizer.save_pretrained(\"./mbert-xquad/final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "NJDhvU71brEL",
        "outputId": "335bb25c-51b9-4fcf-f21a-8b8e3196b832"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1295125582.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3286' max='3286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3286/3286 13:27, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.391200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.737500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.598900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.526300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.415700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.273900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.203800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.867100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.770500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.688700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.720600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.618400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.632300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='179' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [179/179 00:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation: {'eval_runtime': 11.2154, 'eval_samples_per_second': 127.147, 'eval_steps_per_second': 15.96, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./mbert-xquad/final/tokenizer_config.json',\n",
              " './mbert-xquad/final/special_tokens_map.json',\n",
              " './mbert-xquad/final/vocab.txt',\n",
              " './mbert-xquad/final/added_tokens.json',\n",
              " './mbert-xquad/final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/mbert_xquad_model\"\n",
        "trainer.save_model(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(f\"✅ Model saved permanently to Google Drive: {SAVE_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKv6D0f4cOqQ",
        "outputId": "4333bec3-daa4-4d06-9244-baa791d3f5e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Model saved permanently to Google Drive: /content/drive/MyDrive/mbert_xquad_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(SAVE_DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "_HytfHp0i4yv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "context = \"El Taj Mahal está en la ciudad de Agra, India.\"\n",
        "question = \"¿En qué ciudad está el Taj Mahal?\"\n",
        "print(qa({\"question\": question, \"context\": context}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUE4V92geM_L",
        "outputId": "79d3fa02-7c7d-4709-a452-8ed2a65a2e91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.9206821701664012, 'start': 34, 'end': 38, 'answer': 'Agra'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}